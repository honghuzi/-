# 卷积神经网络笔记
## 定义/性质
一种前馈神经网络。即在它内部，参数从输入层向输出层单向传播。与之相反的，递归神经网络内部分构成有向环。

## 结构
1. 卷积层
2. 线性整流层（ReLU layer）
3. 池化层（Pooling layer）
4. 损失函数层


## 核、滤波器
1. 多个滤波器叠加形成卷积层
2. 在数学上的卷积还有一个沿两个方向翻转的过程，但是在这里没有
3. 每个滤波矩阵和图像中的小部分做内积，最终组成一个卷积

## 卷积层影响因素
1. 滤波器个数， 又叫深度 depth
2.  步长 stride
3. zero-padding。外面加上 padding，使总步长能整除，同时图像大小不变

## 新层的大小

## CNN 特点
1. 局部感知
2. 参数共享。 缓解过拟合。
3. 时间或空间亚采样

## 用处和优点
1. 用于识别位移、缩放和及其它扭曲不变性的二维图形。
2. 由于权值矩阵共享，可并行学习。更简单，适应性更强
3. 不用特征抽取，而是隐式地从训练数据中学习。

## 激励函数
非线性激活函数一般有sigmoid、 tanh、 relu等。前两者常用于全连接层，后者用于卷积层。

一个简单的应用是 *逻辑与*。

## ReLU激励层
- sigmoid 容易饱和（在饱和区附近，变换太缓慢，层数趋于 0，会造成信息丢失），造成终止梯度传递，且没有 0 中心化，计算量也大。
- ReLU 收敛快，求梯度简单。
- ReLU 使部分神经元输出为 0，造成了网络的稀疏性，减少了参数的相互依存关系，也缓解了过拟合。
-  如果只有线性激励函数，也不行，无论有多少层，输出都只是输入的线性组合，与没有隐藏层效果相当，相当于 _感知机_ 。而加入非线性激励函数可以逼近任意函数。

## 训练过程

### 前向传播
1. 取样、输入
2. 计算相应的实际输出

### 反向传播
1. 算实际输出与相应理想输出的差
2. 按极小化误差的方法反向传播调整参数矩阵

## 残差网络 (Residual Network， ResNet)

### 作用
对于较深的神经网络而言, 残差能使训练变得容易、提高效率 。误差不会加大。可用于解决梯度消失和梯度爆炸问题。
### 实现
 残差块，把一层的激励值加到另一层上面。
 
# CNN 识别方式
1. 存储一个标准的形状
2. 用 CNN 和它做一个局部一个局部的对比。
